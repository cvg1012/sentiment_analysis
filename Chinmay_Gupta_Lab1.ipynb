{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Chinmay_Gupta_Lab1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBysdv6p5fhb"
      },
      "source": [
        "# Sentiment Analysis on facebook comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BuoqxNY5xbW"
      },
      "source": [
        "### Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIahvs_DXRhO"
      },
      "source": [
        "#Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "#PyTorch\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPHzh3TQ52U4"
      },
      "source": [
        "### Mounting google drive and importing data to pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiSDwjbXWkvm",
        "outputId": "537093f2-9484-4313-8eed-dc6e6385dc1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "df_train=pd.read_csv(\"/content/drive/My Drive/facebook_comments.csv\", header=None, names=['text','sentiment'], encoding='iso-8859-1',lineterminator='\\n')\n",
        "df_train.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Heres a single  to add  to Kindle. Just read t...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>If you tire of Non-Fiction.. Check out http://...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ghost of Round Island is supposedly nonfiction.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Maria:  Do you mean the Nook?  Be careful  bo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0  Heres a single  to add  to Kindle. Just read t...    neutral\n",
              "1  If you tire of Non-Fiction.. Check out http://...    neutral\n",
              "2   Ghost of Round Island is supposedly nonfiction.     neutral\n",
              "3  Why is Barnes and Nobles version of the Kindle...   negative\n",
              "4  @Maria:  Do you mean the Nook?  Be careful  bo...   positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQxM3yJI6DWo"
      },
      "source": [
        "### Creating new labels column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE8PJ9FTXQeP",
        "outputId": "a1a27c5b-e8ba-46eb-bea6-32104e3a49b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "sent={'positive':2,'neutral':1,'negative':0}\n",
        "df_train['labels']=df_train['sentiment'].str.strip().map(sent)\n",
        "df_train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Heres a single  to add  to Kindle. Just read t...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>If you tire of Non-Fiction.. Check out http://...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ghost of Round Island is supposedly nonfiction.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Maria:  Do you mean the Nook?  Be careful  bo...</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment  labels\n",
              "0  Heres a single  to add  to Kindle. Just read t...    neutral       1\n",
              "1  If you tire of Non-Fiction.. Check out http://...    neutral       1\n",
              "2   Ghost of Round Island is supposedly nonfiction.     neutral       1\n",
              "3  Why is Barnes and Nobles version of the Kindle...   negative       0\n",
              "4  @Maria:  Do you mean the Nook?  Be careful  bo...   positive       2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gonVZ_QYzL9",
        "outputId": "d2bc1a9d-0d44-4254-a59c-4ed0761a42a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "training_texts = df_train.text.values\n",
        "labels = df_train.labels.values\n",
        "print(labels.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MinkzIgU6L-T"
      },
      "source": [
        "### Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyrKaXP0ZFZv",
        "outputId": "177fcab4-5cb6-4877-9041-1c0c1c15d330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#Created a vector of 500 input features\n",
        "vectorizer=TfidfVectorizer(stop_words='english', max_features=500)\n",
        "instances = vectorizer.fit_transform(training_texts)\n",
        "X=instances\n",
        "Y=labels\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 500)\n",
            "(1999,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TfGLqCA6gfZ"
      },
      "source": [
        "### Random Forest Model for Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKOSWrBjZ1sb",
        "outputId": "104bd766-ac26-40cd-968b-a66bae0d21d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state = 1234)\n",
        "rf_model = RandomForestClassifier(criterion='entropy', max_depth=2, random_state=1234)\n",
        "rf_cvscores=[]\n",
        "\n",
        "for train_idx,test_idx in kfold.split(X):\n",
        "  rf_model.fit(X[train_idx],Y[train_idx])\n",
        "  acc = rf_model.score(X[test_idx],Y[test_idx])\n",
        "  rf_cvscores.append(acc)\n",
        "\n",
        "print(\"Random Forest - mean: %.4f%% (std: +/- %.4f%%)\"% (np.mean(rf_cvscores)*100,np.std(rf_cvscores)*100))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest - mean: 64.1304% (std: +/- 2.8070%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwzZlbmOc5k8"
      },
      "source": [
        "### Fully Connected Feed forward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLSEJ7ckdHXt"
      },
      "source": [
        "#Hyperparameters\n",
        "epochs = 75\n",
        "lr = 1e-3\n",
        "indim = X.shape[1]\n",
        "outdim = 3\n",
        "drate = 0.7\n",
        "batch_size = 16\n",
        "\n",
        "#Created tensor objects\n",
        "X_tensor = torch.from_numpy(X.toarray())\n",
        "Y_tensor = torch.from_numpy(Y)\n",
        "\n",
        "#Created tensor dataset\n",
        "dataset = TensorDataset(X_tensor,Y_tensor)\n",
        "\n",
        "#Splitting data into training and validation\n",
        "train_size = int(0.8*len(dataset))\n",
        "val_size =  len(dataset)-train_size\n",
        "train_dataset,val_dataset = torch.utils.data.random_split(dataset,[train_size,val_size])\n",
        "\n",
        "#Created DataLoader objects for training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size= batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size= batch_size, shuffle=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crdqGL4W5dDO"
      },
      "source": [
        "### FFN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayhaW1R0eOZs"
      },
      "source": [
        "class SentimetNetwork(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, dropout_rate):\n",
        "    super(SentimetNetwork,self).__init__()\n",
        "    self.fc1=nn.Linear(input_dim,1024)      #input layer\n",
        "    self.fc2=nn.Linear(1024,512)            #hidden layer 1\n",
        "    self.fc3=nn.Linear(512,256)             #hidden layer 2\n",
        "    self.fc4=nn.Linear(256,128)             #hidden layer 3\n",
        "    self.fc5=nn.Linear(128,64)              #hidden layer 4\n",
        "    self.fc6=nn.Linear(64,outdim)           #output layer \n",
        "\n",
        "    self.do1 = nn.Dropout(p=dropout_rate, inplace=False)  #Dropout1 \n",
        "    self.do2 = nn.Dropout(p=dropout_rate, inplace=False)  #Dropout2\n",
        "    self.do3 = nn.Dropout(p=dropout_rate, inplace=False)  #Dropout3\n",
        "    self.do4 = nn.Dropout(p=dropout_rate, inplace=False)  #Dropout4\n",
        "    \n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.fc1(x))       #Using relu as activation function\n",
        "    x = self.do1(x)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.do2(x)\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = self.do3(x)\n",
        "    x = F.relu(self.fc4(x))\n",
        "    x = self.do4(x)\n",
        "    x = F.relu(self.fc5(x))\n",
        "    x = self.fc6(x)\n",
        "\n",
        "    return F.softmax(x)           "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1hw2iQSPjjh",
        "outputId": "14d13cb6-9bc4-4d77-c313-8b1b26b6436a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "#FFN model\n",
        "model = SentimetNetwork(indim, outdim, drate)\n",
        "print(model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimetNetwork(\n",
            "  (fc1): Linear(in_features=500, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc4): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc5): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc6): Linear(in_features=64, out_features=3, bias=True)\n",
            "  (do1): Dropout(p=0.7, inplace=False)\n",
            "  (do2): Dropout(p=0.7, inplace=False)\n",
            "  (do3): Dropout(p=0.7, inplace=False)\n",
            "  (do4): Dropout(p=0.7, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yuytq7F7r58"
      },
      "source": [
        "#Defined optimizer and loss_function  \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
        "criterion = nn.CrossEntropyLoss() "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K51WXeg90lNp"
      },
      "source": [
        "#Defined a function to calculate accuracy\n",
        "def accuracy(y, y_pred):\n",
        "  pred = torch.argmax(y_pred, dim=1)\n",
        "  pred1 = pred.detach().numpy() #converting tensor object to numpy object\n",
        "  y1 = y.detach().numpy() #converting tensor object to numpy object\n",
        "  return accuracy_score(y1,pred1, normalize=True)\n",
        "\n",
        "#Defined a training process function\n",
        "def train(model, train_loader, optimizer, criterion):\n",
        "  \n",
        "  epoch_loss, epoch_acc = 0.0,0.0 # the loss and accuracy for each epoch\n",
        "  model.train()\n",
        "  \n",
        "  for batch_x, batch_y in train_loader:\n",
        "    # Forward pass\n",
        "    y_pred = model(batch_x.float())\n",
        "    loss = criterion(y_pred, batch_y)\n",
        "    acc = accuracy(batch_y, y_pred)\n",
        "    \n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    #Calculate Loss and Accuray for each epoch and aggregate it\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc\n",
        "    \n",
        "    #Calcultae mean of epoch_loss and epoch_acc\n",
        "    epoch_acc_mean = epoch_acc/len(train_loader) #len(train_loader)~0.8*1999/batch_size\n",
        "    epoch_loss_mean = epoch_loss/len(train_loader) \n",
        "\n",
        "  return epoch_loss_mean, epoch_acc_mean\n",
        "\n",
        "# defined a validation/evaluation process function\n",
        "def evaluate(model, val_loader, criterion):\n",
        "  epoch_loss, epoch_acc = 0.0,0.0 # the loss and accuracy for each epoch\n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad(): #we dont optimize in validation function\n",
        "    for batch_x, batch_y in val_loader:\n",
        "        y_pred = model(batch_x.float()) \n",
        "        loss = criterion(y_pred, batch_y)\n",
        "        acc = accuracy(batch_y,y_pred)\n",
        "\n",
        "        #Calculate Loss and Accuray for each epoch and aggregate it\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc \n",
        "        \n",
        "        #Calcultae mean of epoch_loss and epoch_acc\n",
        "        epoch_loss_mean = epoch_loss/len(val_loader) #len(val_loader)~0.2*1999/batch_size\n",
        "        epoch_acc_mean = epoch_acc/len(val_loader)\n",
        "\n",
        "  return epoch_loss_mean, epoch_acc_mean"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwJ7KRjhtuqm",
        "outputId": "c5b1d4b9-5716-48cd-d6f4-7ff7155c48f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#real training and evaluation process\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "  valid_loss, valid_acc =  evaluate(model, val_loader, criterion)\n",
        "\n",
        "  print(f'epoch: {epoch+1:02}')\n",
        "  print(f'\\tTrain Loss: {train_loss: .4f} | Train Acc: {train_acc: .4f}') \n",
        "  print(f'\\tVal Loss: {valid_loss: .4f} | Val Acc: {valid_acc: .4f}')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 01\n",
            "\tTrain Loss:  0.9416 | Train Acc:  0.6423\n",
            "\tVal Loss:  0.9134 | Val Acc:  0.6375\n",
            "epoch: 02\n",
            "\tTrain Loss:  0.8679 | Train Acc:  0.6423\n",
            "\tVal Loss:  0.8071 | Val Acc:  0.6375\n",
            "epoch: 03\n",
            "\tTrain Loss:  0.7577 | Train Acc:  0.8100\n",
            "\tVal Loss:  0.7282 | Val Acc:  0.8200\n",
            "epoch: 04\n",
            "\tTrain Loss:  0.6950 | Train Acc:  0.8580\n",
            "\tVal Loss:  0.7195 | Val Acc:  0.8275\n",
            "epoch: 05\n",
            "\tTrain Loss:  0.6835 | Train Acc:  0.8692\n",
            "\tVal Loss:  0.7168 | Val Acc:  0.8350\n",
            "epoch: 06\n",
            "\tTrain Loss:  0.6706 | Train Acc:  0.8800\n",
            "\tVal Loss:  0.7039 | Val Acc:  0.8475\n",
            "epoch: 07\n",
            "\tTrain Loss:  0.6715 | Train Acc:  0.8794\n",
            "\tVal Loss:  0.7021 | Val Acc:  0.8500\n",
            "epoch: 08\n",
            "\tTrain Loss:  0.6635 | Train Acc:  0.8875\n",
            "\tVal Loss:  0.7140 | Val Acc:  0.8350\n",
            "epoch: 09\n",
            "\tTrain Loss:  0.6650 | Train Acc:  0.8868\n",
            "\tVal Loss:  0.7008 | Val Acc:  0.8500\n",
            "epoch: 10\n",
            "\tTrain Loss:  0.6680 | Train Acc:  0.8831\n",
            "\tVal Loss:  0.6978 | Val Acc:  0.8550\n",
            "epoch: 11\n",
            "\tTrain Loss:  0.6600 | Train Acc:  0.8893\n",
            "\tVal Loss:  0.6984 | Val Acc:  0.8525\n",
            "epoch: 12\n",
            "\tTrain Loss:  0.6574 | Train Acc:  0.8936\n",
            "\tVal Loss:  0.7053 | Val Acc:  0.8475\n",
            "epoch: 13\n",
            "\tTrain Loss:  0.6556 | Train Acc:  0.8961\n",
            "\tVal Loss:  0.6968 | Val Acc:  0.8550\n",
            "epoch: 14\n",
            "\tTrain Loss:  0.6558 | Train Acc:  0.8955\n",
            "\tVal Loss:  0.6926 | Val Acc:  0.8600\n",
            "epoch: 15\n",
            "\tTrain Loss:  0.6527 | Train Acc:  0.8981\n",
            "\tVal Loss:  0.6875 | Val Acc:  0.8650\n",
            "epoch: 16\n",
            "\tTrain Loss:  0.6539 | Train Acc:  0.8961\n",
            "\tVal Loss:  0.6845 | Val Acc:  0.8675\n",
            "epoch: 17\n",
            "\tTrain Loss:  0.6528 | Train Acc:  0.8987\n",
            "\tVal Loss:  0.6757 | Val Acc:  0.8750\n",
            "epoch: 18\n",
            "\tTrain Loss:  0.6501 | Train Acc:  0.9005\n",
            "\tVal Loss:  0.6878 | Val Acc:  0.8625\n",
            "epoch: 19\n",
            "\tTrain Loss:  0.6516 | Train Acc:  0.8992\n",
            "\tVal Loss:  0.6776 | Val Acc:  0.8725\n",
            "epoch: 20\n",
            "\tTrain Loss:  0.6462 | Train Acc:  0.9035\n",
            "\tVal Loss:  0.6786 | Val Acc:  0.8725\n",
            "epoch: 21\n",
            "\tTrain Loss:  0.6477 | Train Acc:  0.9036\n",
            "\tVal Loss:  0.6730 | Val Acc:  0.8775\n",
            "epoch: 22\n",
            "\tTrain Loss:  0.6516 | Train Acc:  0.8972\n",
            "\tVal Loss:  0.6729 | Val Acc:  0.8775\n",
            "epoch: 23\n",
            "\tTrain Loss:  0.6475 | Train Acc:  0.9038\n",
            "\tVal Loss:  0.6705 | Val Acc:  0.8825\n",
            "epoch: 24\n",
            "\tTrain Loss:  0.6498 | Train Acc:  0.9006\n",
            "\tVal Loss:  0.6724 | Val Acc:  0.8775\n",
            "epoch: 25\n",
            "\tTrain Loss:  0.6479 | Train Acc:  0.9011\n",
            "\tVal Loss:  0.6679 | Val Acc:  0.8800\n",
            "epoch: 26\n",
            "\tTrain Loss:  0.6471 | Train Acc:  0.9031\n",
            "\tVal Loss:  0.6804 | Val Acc:  0.8675\n",
            "epoch: 27\n",
            "\tTrain Loss:  0.6451 | Train Acc:  0.9044\n",
            "\tVal Loss:  0.6679 | Val Acc:  0.8850\n",
            "epoch: 28\n",
            "\tTrain Loss:  0.6527 | Train Acc:  0.8981\n",
            "\tVal Loss:  0.6700 | Val Acc:  0.8825\n",
            "epoch: 29\n",
            "\tTrain Loss:  0.6495 | Train Acc:  0.8994\n",
            "\tVal Loss:  0.6588 | Val Acc:  0.8950\n",
            "epoch: 30\n",
            "\tTrain Loss:  0.6501 | Train Acc:  0.9000\n",
            "\tVal Loss:  0.6654 | Val Acc:  0.8850\n",
            "epoch: 31\n",
            "\tTrain Loss:  0.6462 | Train Acc:  0.9037\n",
            "\tVal Loss:  0.6637 | Val Acc:  0.8875\n",
            "epoch: 32\n",
            "\tTrain Loss:  0.6459 | Train Acc:  0.9056\n",
            "\tVal Loss:  0.6633 | Val Acc:  0.8875\n",
            "epoch: 33\n",
            "\tTrain Loss:  0.6451 | Train Acc:  0.9043\n",
            "\tVal Loss:  0.6671 | Val Acc:  0.8850\n",
            "epoch: 34\n",
            "\tTrain Loss:  0.6435 | Train Acc:  0.9075\n",
            "\tVal Loss:  0.6677 | Val Acc:  0.8825\n",
            "epoch: 35\n",
            "\tTrain Loss:  0.6489 | Train Acc:  0.9011\n",
            "\tVal Loss:  0.6685 | Val Acc:  0.8850\n",
            "epoch: 36\n",
            "\tTrain Loss:  0.6462 | Train Acc:  0.9043\n",
            "\tVal Loss:  0.6816 | Val Acc:  0.8675\n",
            "epoch: 37\n",
            "\tTrain Loss:  0.6513 | Train Acc:  0.8986\n",
            "\tVal Loss:  0.6736 | Val Acc:  0.8750\n",
            "epoch: 38\n",
            "\tTrain Loss:  0.6507 | Train Acc:  0.9005\n",
            "\tVal Loss:  0.6610 | Val Acc:  0.8950\n",
            "epoch: 39\n",
            "\tTrain Loss:  0.6491 | Train Acc:  0.9012\n",
            "\tVal Loss:  0.6809 | Val Acc:  0.8700\n",
            "epoch: 40\n",
            "\tTrain Loss:  0.6467 | Train Acc:  0.9031\n",
            "\tVal Loss:  0.6690 | Val Acc:  0.8850\n",
            "epoch: 41\n",
            "\tTrain Loss:  0.6512 | Train Acc:  0.8987\n",
            "\tVal Loss:  0.6878 | Val Acc:  0.8650\n",
            "epoch: 42\n",
            "\tTrain Loss:  0.6474 | Train Acc:  0.9037\n",
            "\tVal Loss:  0.6691 | Val Acc:  0.8825\n",
            "epoch: 43\n",
            "\tTrain Loss:  0.6486 | Train Acc:  0.9031\n",
            "\tVal Loss:  0.6808 | Val Acc:  0.8700\n",
            "epoch: 44\n",
            "\tTrain Loss:  0.6499 | Train Acc:  0.8993\n",
            "\tVal Loss:  0.6811 | Val Acc:  0.8700\n",
            "epoch: 45\n",
            "\tTrain Loss:  0.6533 | Train Acc:  0.8980\n",
            "\tVal Loss:  0.6752 | Val Acc:  0.8775\n",
            "epoch: 46\n",
            "\tTrain Loss:  0.6476 | Train Acc:  0.9043\n",
            "\tVal Loss:  0.6879 | Val Acc:  0.8625\n",
            "epoch: 47\n",
            "\tTrain Loss:  0.6473 | Train Acc:  0.9037\n",
            "\tVal Loss:  0.6909 | Val Acc:  0.8600\n",
            "epoch: 48\n",
            "\tTrain Loss:  0.6515 | Train Acc:  0.9005\n",
            "\tVal Loss:  0.6886 | Val Acc:  0.8625\n",
            "epoch: 49\n",
            "\tTrain Loss:  0.6488 | Train Acc:  0.9012\n",
            "\tVal Loss:  0.6867 | Val Acc:  0.8650\n",
            "epoch: 50\n",
            "\tTrain Loss:  0.6477 | Train Acc:  0.9031\n",
            "\tVal Loss:  0.6774 | Val Acc:  0.8725\n",
            "epoch: 51\n",
            "\tTrain Loss:  0.6530 | Train Acc:  0.8969\n",
            "\tVal Loss:  0.6814 | Val Acc:  0.8700\n",
            "epoch: 52\n",
            "\tTrain Loss:  0.6482 | Train Acc:  0.9025\n",
            "\tVal Loss:  0.6808 | Val Acc:  0.8700\n",
            "epoch: 53\n",
            "\tTrain Loss:  0.6490 | Train Acc:  0.9018\n",
            "\tVal Loss:  0.6785 | Val Acc:  0.8725\n",
            "epoch: 54\n",
            "\tTrain Loss:  0.6514 | Train Acc:  0.8992\n",
            "\tVal Loss:  0.6793 | Val Acc:  0.8725\n",
            "epoch: 55\n",
            "\tTrain Loss:  0.6476 | Train Acc:  0.9037\n",
            "\tVal Loss:  0.6734 | Val Acc:  0.8800\n",
            "epoch: 56\n",
            "\tTrain Loss:  0.6547 | Train Acc:  0.8962\n",
            "\tVal Loss:  0.6746 | Val Acc:  0.8725\n",
            "epoch: 57\n",
            "\tTrain Loss:  0.6494 | Train Acc:  0.9012\n",
            "\tVal Loss:  0.6774 | Val Acc:  0.8725\n",
            "epoch: 58\n",
            "\tTrain Loss:  0.6520 | Train Acc:  0.8993\n",
            "\tVal Loss:  0.6813 | Val Acc:  0.8700\n",
            "epoch: 59\n",
            "\tTrain Loss:  0.6460 | Train Acc:  0.9049\n",
            "\tVal Loss:  0.6776 | Val Acc:  0.8725\n",
            "epoch: 60\n",
            "\tTrain Loss:  0.6507 | Train Acc:  0.9006\n",
            "\tVal Loss:  0.6692 | Val Acc:  0.8825\n",
            "epoch: 61\n",
            "\tTrain Loss:  0.6546 | Train Acc:  0.8949\n",
            "\tVal Loss:  0.6715 | Val Acc:  0.8800\n",
            "epoch: 62\n",
            "\tTrain Loss:  0.6517 | Train Acc:  0.8999\n",
            "\tVal Loss:  0.6800 | Val Acc:  0.8700\n",
            "epoch: 63\n",
            "\tTrain Loss:  0.6515 | Train Acc:  0.9006\n",
            "\tVal Loss:  0.6869 | Val Acc:  0.8625\n",
            "epoch: 64\n",
            "\tTrain Loss:  0.6555 | Train Acc:  0.8962\n",
            "\tVal Loss:  0.6635 | Val Acc:  0.8875\n",
            "epoch: 65\n",
            "\tTrain Loss:  0.6546 | Train Acc:  0.8956\n",
            "\tVal Loss:  0.6693 | Val Acc:  0.8825\n",
            "epoch: 66\n",
            "\tTrain Loss:  0.6529 | Train Acc:  0.8980\n",
            "\tVal Loss:  0.6654 | Val Acc:  0.8850\n",
            "epoch: 67\n",
            "\tTrain Loss:  0.6519 | Train Acc:  0.8981\n",
            "\tVal Loss:  0.6715 | Val Acc:  0.8800\n",
            "epoch: 68\n",
            "\tTrain Loss:  0.6493 | Train Acc:  0.9006\n",
            "\tVal Loss:  0.6692 | Val Acc:  0.8775\n",
            "epoch: 69\n",
            "\tTrain Loss:  0.6502 | Train Acc:  0.9018\n",
            "\tVal Loss:  0.6820 | Val Acc:  0.8675\n",
            "epoch: 70\n",
            "\tTrain Loss:  0.6523 | Train Acc:  0.8987\n",
            "\tVal Loss:  0.6734 | Val Acc:  0.8775\n",
            "epoch: 71\n",
            "\tTrain Loss:  0.6548 | Train Acc:  0.8955\n",
            "\tVal Loss:  0.6666 | Val Acc:  0.8800\n",
            "epoch: 72\n",
            "\tTrain Loss:  0.6534 | Train Acc:  0.8975\n",
            "\tVal Loss:  0.6635 | Val Acc:  0.8875\n",
            "epoch: 73\n",
            "\tTrain Loss:  0.6501 | Train Acc:  0.9000\n",
            "\tVal Loss:  0.6964 | Val Acc:  0.8550\n",
            "epoch: 74\n",
            "\tTrain Loss:  0.6536 | Train Acc:  0.8969\n",
            "\tVal Loss:  0.6640 | Val Acc:  0.8875\n",
            "epoch: 75\n",
            "\tTrain Loss:  0.6480 | Train Acc:  0.9018\n",
            "\tVal Loss:  0.6590 | Val Acc:  0.8925\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}